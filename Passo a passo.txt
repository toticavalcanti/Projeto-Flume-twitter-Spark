Passo a passo

1 - Criar o diretório:
mkdir -p /usr/lib/flume-ng/plugins.d/twitter-streaming/lib/

2 - Criar o diretório:
mkdir -p /var/lib/flume-ng/plugins.d/twitter-streaming/lib/

3 - Colocar o arquivo flume-sources-1.0-SNAPSHOT.jar nos diretórios criados anteriormente, para isso, você deve estar dentro da pasta está tem o flume-sources-1.0-SNAPSHOT.jar, no meu caso eu tô na pasta /datasets/ e dentro dela tem o flume-sources-1.0-SNAPSHOT.jar que foi feito o build a partir do repositório do Gihub que indiquei: https://github.com/cloudera/cdh-twitter-example:

Copiar para:
cp flume-sources-1.0-SNAPSHOT.jar /usr/lib/flume-ng/plugins.d/twitter-streaming/lib/

Copiar também para:
cp /var/lib/flume-ng/plugins.d/twitter-streaming/lib/

4 - Copie para a pasta /etc/flume-ng/conf.empty o arquivo de configuração do Flume para a app do twitter: flume_process_twitter.conf, se você já tiver na pasta onde ele está é só usar:
cp flume_process_twitter.conf /etc/flume-ng/conf.empty/

5 - Antes de rodar o agente, crie a pasta no HDFS que você configurou no flume_process_twitter.conf, para o agente colocar os arquivos coletados:
hadoop fs -mkdir -p /twitteranalytics/

6 - Colocar o arquivo hive-serdes-1.0-SNAPSHOT.jar na pasta /usr/lib/hive/lib, se você tiver na pasta dele é só usar:
sudo cp hive-serdes-1.0-SNAPSHOT.jar /usr/lib/hive/lib

Dê um:
sudo service hive-server2 stop
Depois:
sudo service hive-server2 start

7 - Crie as tabelas Hive com o esquema Create_Twitter_Schema.hql
hive -f Create_Twitter_Schema.hql

8 - Adicionar a linha abaixo em /etc/hosts da máquina Cloudera no final do arquivo
199.59.148.138 stream.twitter.com

9 - Agora é só rodar o agente flume pra pegar os arquivos do twitter, para isso entre na pasta /etc/flume-ng/conf.empty com o comando:
cd /etc/flume-ng/conf.empty/

Agora digite o comando abaixo para iniciar o agente:
flume-ng agent -f flume_process_twitter.conf Dflume.root.logger=DEBUG,console -n TwitterAgent

Pronto o agente vai coletar os dados do twitter

10 - O agente vai começar a coletar os dados, depois você poderá ver os arquivos gerados no HDFS usando:
hadoop fs -get /user/flume/tweets/2018/03/29/11/FlumeData.1522349293383

Perceba que ele gera arquivos por pastas separadas por data %Y/%m/%d/%H/ como configurado no agente.







